---
title: PythonのLLMをopenllmetryで計測する (GCP Cloud Trace)
tags: Python LLM traceloop openllmetry GoogleCloud
author: nakamasato
slide: false
---
## はじめに

LLMの呼び出しがあるAPIの場合、Latencyが大幅に増えるケースが多いので、Traceabilityがとても重要になります。
Performanceだけでなく、実際にLLMがどこでどのように呼ばれてどんな結果が帰っているのかも含めて一連の呼び出しを紐づけるTracingは調査に役立ちます。

## openllmetry

https://github.com/traceloop/openllmetry

> Open-source observability for your LLM application, based on OpenTelemetry

Open-sourceのOpenTelemetryベースのLLM app用のObservabilityツールです。

DatadogやGoogle Cloudをはじめとする多数のExporterがサポートされており、Tracing、Metrics、Loggingを紐づける事ができます。

## Example

今回は、前回紹介した [Flask appをOpentelemetryでGCP Cloud Traceに連携する](https://qiita.com/nakamasato/items/ca7ef5610e1876443cfb)を拡張して、Flaskで作ったLLM appにopenllmetryを入れる例を紹介します。

## コード

```py:main.py
from traceloop.sdk import Traceloop
from opentelemetry.exporter.cloud_trace import CloudTraceSpanExporter
from opentelemetry.instrumentation.flask import FlaskInstrumentor

Traceloop.init(
    app_name="<app name>",
    exporter=CloudTraceSpanExporter(resource_regex="env|version|service.name"),
    resource_attributes={
        "env": os.getenv("ENV", "dev"),
        "version": os.getenv("VERSION", "unknown"),
    },
)


app = Flask(__name__)

FlaskInstrumentor().instrument_app(app)

@app.route("/")
def hello():
    return "Hello!"

if __name__ == "__main__":
    app.run(debug=True)
```

前回設定していた`CloudTraceSpanExporter`を同様にTraceloop.initのexporterに渡してあげるだけでOkです。

これで、flask appの中で使われる openai, anthropicなどのLLMの呼び出し、LangChain、LangGraphの呼び出し部分の計装が自動的に行われます。

## Cloud Trace

この設定をするとCloud Traceで以下のようなTraceが取れるようになります :tada: 

![Screenshot 2025-06-02 at 7.58.03.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/7059/7d63919c-beca-4150-8e8e-c69f69c638ba.png)

:::note warn
このトレースは上のサンプルコードから出たものではありません。
:::

